# Data Frames

1. pandas : rectangular data
```
each label : row name 下面的名字
each variable : column's name下面的名字
methods:
a.dataframe.head().. returns the first few rows (the “head” of the DataFrame).
b.dataframe.info() ... shows information on each of the columns, such as the data type and number of missing values.
c.dataframe.describe()....calculates a few summary statistics for each column.

attributes:
a.dataframe.values ...  A two-dimensional NumPy array of values.
b.dataframe.columns ...  An index of columns: the column names.
c.dataframe.index ...  An index for the rows: either row numbers or row names.
d.dataframe.shape... returns the number of rows and columns of the DataFrame.
e.dataframe.size....returns size
```

2.Sorting and subsetting : subsetting 如果要結果要return 是dataframe ， 要用 double bracket [[]] 。 其他只要一個[]就好，因為一個[]就可以選 column_name 即可達到效果
```
sorting:
df.sort_values("breed") ... one column
df.sort_values(["breed", "weight_kg"]) ....multiple columns
ex : homelessness_reg_fam = homelessness.sort_values(['region','family_members'],ascending=[True,False])
```

```
subsetting columns:
df[["col_a", "col_b"]] ... select multiple coiumns, outer bracket subset df, inner bracket select column names 

subsetting rows: 
3 Steps
1.Select the corresponding column
2.Set conditions on column
3.Use result(To return True values :) to select rows data

dogs[dogs["height_cm"] > 60]
dogs[dogs["color"] == "tan"]
dogs[(dogs["height_cm"] > 60) & (dogs["col_b"] == "tan")]

```
```
Subsetting rows by categorical variables ... select mutiple rows at the same time
3 Steps
1.Select the multiple corresponding column
2.Set conditions on column [use isin() method]
3.Use result(To return True values :) to select rows data
colors = ["brown", "black", "tan"]
condition = dogs["color"].isin(colors)
dogs[condition]
```
```
new columns
1.mutating
2.transforming a dataframe
3.feature engineering
```

# Aggregating data
1.column 的 mean and medium / max and min
```
sales['weekly_sales'].mean()
sales['weekly_sales'].median()
sales['date'].max()
sales['date'].min()
```

2.Efficient summaries 
```
The .agg() method allows you to apply your own custom functions to a DataFrame, and also multiple columns and multiple functions [data type 不會改變]

import numpy as np
def iqr(column):
    return column.quantile(0.75) - column.quantile(0.25)
    
sales[["temperature_c", "fuel_price_usd_per_l", "unemployment"]].agg([iqr,np.median])...outer bracket means dataframe, inner bracket means column name[panda series]

df.groupby('col').agg({'選取的col_name':'function'}) #groupby col下面的variable

```
3. Cumulative statistics
```
1.df.cumsum()
2.df.cummax()
```

4. Counting : variable value counts
```
- drop same instance appeared more than once :
df.drop_duplicates(subset=['cola','colb'])
df[df["cola"]].drop_duplicates(subset='colb') .... 不常見的方法

- Counting categorical variables
df["cola"].value_counts()
df["cola"].value_counts(normalize=True)..proportion
df['cola'].value_counts(sort=True, normalize=True) ... 排序依高到低＋比例顯示

```
5. groupby (for each type 代表要groupby)
```
.groupby('col name')把col name下面每個variable 整理起來
each label : row name 下面的名字
each variable : column's name下面的名字
df.groupby('column name') ... 變成一個panda dataframegroupyby，再去做column操作
df.groupby(['cola','colb'])['colc'].sum()...多個variable的groupby
df.groupby('col_name',as_index=False).... as_index = False 代表column_name 不用當index

ps.[numply float,numpy float, numpy float]也算是ndarray,不算list
ps..groupby() 後的type為pandas dataframegroupyby
ps.groupby()['col name']後的type 為 pandas seriesgroupyby
ps..sum()後的type為numpy.float
ps.pandas and ndarray都可以做element的計算
ps.Multiple column selection can be done with double-brackets,
```

6.pivot_tables :  alternative way of groupby, but more convenient []
```
The values argument of .pivot_table() takes in the name of the column you want to summarize. 選擇做操作的某一個column
The index argument of .pivot_table() takes in the name of the column you want to group by.
The columns argument of .pivot_table() 用以分割數據，去選出想比較的特定欄位。
The fill_value=0 argument of .pivot_table() fill in the missing value
The margins=True argument of .pivot_table() can show the mean of each variable
The aggfunc=[function1,function2]
ex: mean_sales_by_type = sales.pivot_table(values='weekly_sales',index='type')
ex:mean_med_sales_by_type = sales.pivot_table(values="weekly_sales", index="type", aggfunc=[np.mean, np.median])
ex :mean_sales_by_type_holiday = sales.pivot_table(values="weekly_sales", index="type", columns="is_holiday", fill_value=0,margins=True)
```
# slicing and indexing data

1. set/reset a column as an index + sort index
```
ps.index values violates tidy data and it's easier to make mistakes
ps.index make subsetting simpler : The killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.
df.set_index('col_name')
df.set_index(["col_namea",'col_nameb'])
df_ind.reset_index()
df_ind.reset_index(drop=True)
df_ind.loc[['variable_a','variable_b']] ..... subset on outer levels [index一定要是col_name,因為loc可以取 index values subsetting]
df_ind.loc[[('variable_a＿outer_layer','variable_a_inner_layer'),('variable_b＿outer_layer','variable_b_inner_layer')]]..... subset on inner levels,use tuples
df_ind.sort_index()
df_ind.sort_index(level = ['color','breed'],ascending=[True,False]) .... passing list to change ascending
```
2. slicing with .loc and .iloc
```
ps.sort before slicing
slicing with index : .loc若用在subsetting字串，單個bracket 就是dataframe
df_ind_srt.loc["variable_a":"variable_b"]..... subset on outer levels [index一定要是col_name,因為loc可以取 index values subsetting]
df_ind_srt.loc[('variable_a＿outer_layer','variable_a_inner_layer'),('variable_b＿outer_layer','variable_b_inner_layer')].....subset on inner levels, use tuples

slicing with columns :
df_ind_srt.loc[:,"a"]

slicing with both rows and columns:
df_ind_srt.loc[("variable_a＿outer_layer", "variable_a_inner_layer"):("variable_b＿outer_layer", "variable_b_inner_layer"), "col_name_a":"col_name_b"]

slicing with iloc
df.iloc[row_num,col_num])
```
3.slicing with time series
```
ps.an important and common way of using slicing data->subset dataframe by a range of dates ... you can slice by partial dates
df_ind_srt.loc["2010-08":"2011-02"]

ps.You can access the components of a date (year, month and day) using code of the form dataframe["column"].dt.component. For example, the month component is dataframe["column"].dt.month, and the year component is dataframe["column"].dt.year
```

4.working with pivot tables
```
ps.axis="index or columns" means calculate the statistic across rows
df.mean(axis='columns')
```
# creating and  visualizing data
1. histograms
```
import matplotlib.pyplot as plt
df.groupby('col_namea')['col_name_b'].sum()
df.plot(kind='bar')
df.plot(x="col_name_a", y="col_name_b", kind="scatter", title="Number of avocados sold vs. average price")
df.hist(alpha=0.5,bins=20), alpha represents transparency
plt.show()
```
2. missing value
```
df.isna()....check if there is any missing value
df.isna().any()....check if there is any missing value
df.isna().sum()....check if there is any missing value
df.isna().sum().plot(kind='bar'); plt.show() ....check if there is any missing value using bar plot

df.dropna()....drop missing value
df.fillna(數字)...fill in missing value

```
3. creating dataframes 
```
three ways:
1.from a list of dictionary - constructed row by row
avocados_list = [
    {'date': "2019-11-03",'small_sold': 10376832, 'large_sold' : 7835071},
    {'date': "2019-11-10",'small_sold': 10717154, 'large_sold' : 8561348},
]
pd.DataFrame(avocados_list)

2.from a dictionary of lists- constructed column by column
avocados_dict = {
  "date": ["2019-11-17","2019-12-01"],
  "small_sold": [10859987,9291631],
  "large_sold": [7674135,6238096]
}
pd.DataFrame(avocados_dict)

3. CSV to dataframe
pd.read_csv("airline_bumping.csv")
df.to_csv()....dataframe to csv("airline_totals_sorted.csv")

````









































